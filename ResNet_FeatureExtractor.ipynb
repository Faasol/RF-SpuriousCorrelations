{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060f24fb-4516-4a2e-8136-cba94c045aee",
   "metadata": {},
   "source": [
    "# ResNet50 Feature Extractor on CMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf3cdba-8f02-49d2-8f9c-ca102ec0145f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad0f12c8-b9af-4468-ab26-261f4483072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d06b44-3c71-474e-8480-14756c745ced",
   "metadata": {},
   "source": [
    "## Dataset CMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eece6ef9-3acd-41e9-baa8-9ed178fe08b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_bias(folder_path):\n",
    "    X = []\n",
    "    Y = []\n",
    "    Y_bias = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        X.append(img)\n",
    "\n",
    "        parts = filename.split('_')\n",
    "        y = int(parts[1])\n",
    "        y_bias = int(parts[2].split('.')[0])\n",
    "        Y.append(y)\n",
    "        Y_bias.append(y_bias)\n",
    "\n",
    "    return np.array(X), np.array(Y), np.array(Y_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723841b-20fd-4671-b8da-2d114ba48bf3",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0009b724-2769-4bd5-b88a-c93f147273c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'cmnist/5pct/'\n",
    "numbers = [str(i) + \"/\" for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d60ce0f-7670-4b3d-93b1-f80eb05753a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "Y_train_bias = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9ab9e6-94bf-4f18-ae4e-8987a0ecf7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for parent_folder in ['align/', 'conflict/']:\n",
    "    for number in numbers:\n",
    "        X_batch, Y_batch, Y_bias_batch = load_images_bias(folder_path + parent_folder + number)\n",
    "        X_train.extend(X_batch)        \n",
    "        Y_train.extend(Y_batch)\n",
    "        Y_train_bias.extend(Y_bias_batch)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_train_bias = np.array(Y_train_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb092c5-54fe-4d26-85fe-285de8512e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape      = (55000, 28, 28, 3)\n",
      "Y_train shape      = (55000,)\n",
      "Y_train_bias shape = (55000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape      = {X_train.shape}\")\n",
    "print(f\"Y_train shape      = {Y_train.shape}\")\n",
    "print(f\"Y_train_bias shape = {Y_train_bias.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2854396a-202b-4c37-a8ba-bc83e2fa7ca5",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a68c6dbe-7ca1-4ecc-bfa5-1d9bf0ee3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = []\n",
    "Y_valid = []\n",
    "Y_valid_bias = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f21a18cb-efb1-4d50-8460-7ea44ae23205",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'cmnist/5pct/valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e55fc88-6a5a-4466-ad5f-bc42caa63981",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch, Y_batch, Y_bias_batch = load_images_bias(folder_path)\n",
    "X_valid.extend(X_batch)        \n",
    "Y_valid.extend(Y_batch)\n",
    "Y_valid_bias.extend(Y_bias_batch)\n",
    "\n",
    "X_valid = np.array(X_valid)\n",
    "Y_valid = np.array(Y_valid)\n",
    "Y_valid_bias = np.array(Y_valid_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85b3da1f-1dd6-48e0-8aa0-2d1bddb2d579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_valid shape      = (5000, 28, 28, 3)\n",
      "Y_valid shape      = (5000,)\n",
      "Y_valid_bias shape = (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_valid shape      = {X_valid.shape}\")\n",
    "print(f\"Y_valid shape      = {Y_valid.shape}\")\n",
    "print(f\"Y_valid_bias shape = {Y_valid_bias.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1e8cb3-aa4c-4796-a667-e094d893bc48",
   "metadata": {},
   "source": [
    "## Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56b71583-fbb1-4fe0-9b84-b812ecd7b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "Y_test = []\n",
    "Y_test_bias = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4517bd84-20fd-49a8-b04a-7757801d28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'cmnist/test/'\n",
    "\n",
    "for number in numbers:\n",
    "    X_batch, Y_batch, Y_bias_batch = load_images_bias(folder_path + number)\n",
    "    X_test.extend(X_batch)        \n",
    "    Y_test.extend(Y_batch)\n",
    "    Y_test_bias.extend(Y_bias_batch)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "Y_test_bias = np.array(Y_test_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f47c294c-444c-4c61-8b37-a299ac8c45b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape      = (10000, 28, 28, 3)\n",
      "Y_test shape      = (10000,)\n",
      "Y_test_bias shape = (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_test shape      = {X_test.shape}\")\n",
    "print(f\"Y_test shape      = {Y_test.shape}\")\n",
    "print(f\"Y_test_bias shape = {Y_test_bias.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641d67a-3391-49f4-bf78-deab5fd48206",
   "metadata": {},
   "source": [
    "## Resize, to Tensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80f12a49-c28b-48a7-9349-909c5103fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_transforms = T.Compose([\n",
    "    # 1. Ridimensiona l'immagine da 28x28 a 224x224. I 3 canali vengono mantenuti.\n",
    "    T.Resize((224, 224)),\n",
    "    \n",
    "    # 2. Converte l'immagine in un Tensore PyTorch.\n",
    "    T.ToTensor(),\n",
    "    \n",
    "    # 3. Normalizza il tensore usando la media e la deviazione standard di ImageNet.\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202682a2-a039-4ae8-9d28-fa0d16ed1078",
   "metadata": {},
   "source": [
    "## ResNet50 modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bafd8629-90c8-49f9-9d91-a05fb8273fdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "feature_extractor.to(device)\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f8cd3c-0776-4a61-9638-5a9231362a0b",
   "metadata": {},
   "source": [
    "## Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4063b06a-511d-4f79-895f-0cf1a9b2b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5bdaa596-d206-4494-bf47-5166fc2aabe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(X):\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        # The main for-loop iterates through the X array in steps of 'batch_size'\n",
    "        for i in tqdm(range(0, len(X), batch_size), desc=\"Processing Batches\"):\n",
    "            \n",
    "            # 1. Get a small batch of images from X\n",
    "            batch_numpy = X[i : i + batch_size]\n",
    "            \n",
    "            # 2. Transform each image in this small batch\n",
    "            batch_tensor_list = []\n",
    "            for img_numpy in batch_numpy:\n",
    "                img_pil = Image.fromarray(img_numpy)\n",
    "                transformed_tensor = imagenet_transforms(img_pil)\n",
    "                batch_tensor_list.append(transformed_tensor)\n",
    "            \n",
    "            # 3. Stack the list of transformed tensors into a single batch tensor\n",
    "            batch_to_process = torch.stack(batch_tensor_list)\n",
    "            \n",
    "            # 4. Move the batch to the correct device and pass it to the model\n",
    "            batch_to_process = batch_to_process.to(device)\n",
    "            features = feature_extractor(batch_to_process)\n",
    "            \n",
    "            # 5. Move the results to the CPU, convert to NumPy, and store them\n",
    "            features_np = features.squeeze().cpu().numpy()\n",
    "            # Use .extend() to add each feature vector from the batch to our main list\n",
    "            result.extend(features_np)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc9453-2b14-4716-8206-b777746f797c",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ef6bde3-53b9-4c7a-8313-216793bff1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|███████████████████| 1719/1719 [13:23<00:00,  2.14it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_features = np.array(extract_features(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b25f627-88ec-49f7-b216-e6dd55354e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_features shape: (55000, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train_features shape: {X_train_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b38047-3e2f-4278-9fd6-40eb219e30f7",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "afade5b0-fcf3-4a92-aa4d-e55281d6a7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|█████████████████████| 157/157 [01:03<00:00,  2.46it/s]\n"
     ]
    }
   ],
   "source": [
    "X_valid_features = np.array(extract_features(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a3913d0-6c5f-4cad-bb07-a358702bebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_valid_features shape: (5000, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_valid_features shape: {X_valid_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cebf70f-f4d6-4af1-99cd-d365f5ba6b4c",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "82f68ffe-20ab-4bde-8544-9b2e9b3579c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|█████████████████████| 313/313 [02:46<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_features = np.array(extract_features(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65de6e7f-93b5-46ef-925e-f1d3f41751c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_features shape: (10000, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_test_features shape: {X_test_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d1a30-1065-45ce-9c40-d6f217ff3d08",
   "metadata": {},
   "source": [
    "## Save Data to .npy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ac359a5-94b6-47cc-a193-d5e11e84e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"features_RN/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded46872-4ff0-4570-8f9e-b04479bda1b7",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "97d13b1d-7000-4eba-a2f8-1fb80771e4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array shapes:\n",
      "X_train shape: (55000, 2048)\n",
      "Y_train shape: (55000,)\n",
      "Y_train_bias shape: (55000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original array shapes:\")\n",
    "print(f\"X_train shape: {X_train_features.shape}\")\n",
    "print(f\"Y_train shape: {Y_train.shape}\")\n",
    "print(f\"Y_train_bias shape: {Y_train_bias.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "13b3c97a-5a76-4650-ab7f-024b005271ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'x_train.npy' saved successfully.\n",
      "'y_train.npy' saved successfully.\n",
      "'b_train.npy' saved successfully.\n"
     ]
    }
   ],
   "source": [
    "np.save(folder_path + 'x_train.npy', X_train_features)\n",
    "print(\"'x_train.npy' saved successfully.\")\n",
    "\n",
    "np.save(folder_path + 'y_train.npy', Y_train)\n",
    "print(\"'y_train.npy' saved successfully.\")\n",
    "\n",
    "np.save(folder_path + 'b_train.npy', Y_train_bias)\n",
    "print(\"'b_train.npy' saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "00075375-384b-4e5a-a080-9d871c05df33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading arrays back from .npy files to verify...\n",
      "\n",
      "Shapes of loaded arrays:\n",
      "Shape of loaded X: (55000, 2048)\n",
      "Shape of loaded Y: (55000,)\n",
      "Shape of loaded Y_bias: (55000,)\n",
      "\n",
      "Verification successful: Original and loaded arrays are identical.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading arrays back from .npy files to verify...\")\n",
    "\n",
    "X_loaded = np.load(folder_path + 'x_train.npy')\n",
    "Y_loaded = np.load(folder_path + 'y_train.npy')\n",
    "Y_bias_loaded = np.load(folder_path + 'b_train.npy')\n",
    "\n",
    "print(\"\\nShapes of loaded arrays:\")\n",
    "print(f\"Shape of loaded X: {X_loaded.shape}\")\n",
    "print(f\"Shape of loaded Y: {Y_loaded.shape}\")\n",
    "print(f\"Shape of loaded Y_bias: {Y_bias_loaded.shape}\")\n",
    "\n",
    "assert np.array_equal(X_train_features, X_loaded) and np.array_equal(Y_train, Y_loaded) and np.array_equal(Y_train_bias, Y_bias_loaded)\n",
    "print(\"\\nVerification successful: Original and loaded arrays are identical.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523b1bb-5d50-4ebc-859f-a7392adba252",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0a294802-0972-4e96-94dc-262f4f1bbcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array shapes:\n",
      "X_train shape: (5000, 2048)\n",
      "Y_train shape: (5000,)\n",
      "Y_train_bias shape: (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original array shapes:\")\n",
    "print(f\"X_train shape: {X_valid_features.shape}\")\n",
    "print(f\"Y_train shape: {Y_valid.shape}\")\n",
    "print(f\"Y_train_bias shape: {Y_valid_bias.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7f16b236-a43e-4454-b482-d99f3af55b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'x_val.npy' saved successfully.\n",
      "'y_val.npy' saved successfully.\n",
      "'b_val.npy' saved successfully.\n"
     ]
    }
   ],
   "source": [
    "np.save(folder_path + 'x_val.npy', X_valid_features)\n",
    "print(\"'x_val.npy' saved successfully.\")\n",
    "\n",
    "np.save(folder_path + 'y_val.npy', Y_valid)\n",
    "print(\"'y_val.npy' saved successfully.\")\n",
    "\n",
    "np.save(folder_path + 'b_val.npy', Y_valid_bias)\n",
    "print(\"'b_val.npy' saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b390b054-bef4-4763-abe9-0889785bc2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading arrays back from .npy files to verify...\n",
      "\n",
      "Shapes of loaded arrays:\n",
      "Shape of loaded X: (5000, 2048)\n",
      "Shape of loaded Y: (5000,)\n",
      "Shape of loaded Y_bias: (5000,)\n",
      "\n",
      "Verification successful: Original and loaded arrays are identical.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading arrays back from .npy files to verify...\")\n",
    "\n",
    "X_loaded = np.load(folder_path + 'x_val.npy')\n",
    "Y_loaded = np.load(folder_path + 'y_val.npy')\n",
    "Y_bias_loaded = np.load(folder_path + 'b_val.npy')\n",
    "\n",
    "print(\"\\nShapes of loaded arrays:\")\n",
    "print(f\"Shape of loaded X: {X_loaded.shape}\")\n",
    "print(f\"Shape of loaded Y: {Y_loaded.shape}\")\n",
    "print(f\"Shape of loaded Y_bias: {Y_bias_loaded.shape}\")\n",
    "\n",
    "assert np.array_equal(X_valid_features, X_loaded) and np.array_equal(Y_valid, Y_loaded) and np.array_equal(Y_valid_bias, Y_bias_loaded)\n",
    "print(\"\\nVerification successful: Original and loaded arrays are identical.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d75701f-ba18-4c9f-b360-8cbf55ac9a64",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "271810ea-5b28-4ca8-afec-6107fa4b2a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array shapes:\n",
      "X_train shape: (10000, 2048)\n",
      "Y_train shape: (10000,)\n",
      "Y_train_bias shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original array shapes:\")\n",
    "print(f\"X_train shape: {X_test_features.shape}\")\n",
    "print(f\"Y_train shape: {Y_test.shape}\")\n",
    "print(f\"Y_train_bias shape: {Y_test_bias.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e62d0bc1-d490-4ee8-9e30-d44b9d644610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'x_test.npy' saved successfully.\n",
      "'y_test.npy' saved successfully.\n",
      "'b_test.npy' saved successfully.\n"
     ]
    }
   ],
   "source": [
    "np.save(folder_path + 'x_test.npy', X_test_features)\n",
    "print(\"'x_test.npy' saved successfully.\")\n",
    "\n",
    "np.save(folder_path + 'y_test.npy', Y_test)\n",
    "print(\"'y_test.npy' saved successfully.\")\n",
    "\n",
    "np.save(folder_path + 'b_test.npy', Y_test_bias)\n",
    "print(\"'b_test.npy' saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ad89f2d5-a39f-45b8-8392-2110787afcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading arrays back from .npy files to verify...\n",
      "\n",
      "Shapes of loaded arrays:\n",
      "Shape of loaded X: (10000, 2048)\n",
      "Shape of loaded Y: (10000,)\n",
      "Shape of loaded Y_bias: (10000,)\n",
      "\n",
      "Verification successful: Original and loaded arrays are identical.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading arrays back from .npy files to verify...\")\n",
    "\n",
    "X_loaded = np.load(folder_path + 'x_test.npy')\n",
    "Y_loaded = np.load(folder_path + 'y_test.npy')\n",
    "Y_bias_loaded = np.load(folder_path + 'b_test.npy')\n",
    "\n",
    "print(\"\\nShapes of loaded arrays:\")\n",
    "print(f\"Shape of loaded X: {X_loaded.shape}\")\n",
    "print(f\"Shape of loaded Y: {Y_loaded.shape}\")\n",
    "print(f\"Shape of loaded Y_bias: {Y_bias_loaded.shape}\")\n",
    "\n",
    "assert np.array_equal(X_test_features, X_loaded) and np.array_equal(Y_test, Y_loaded) and np.array_equal(Y_test_bias, Y_bias_loaded)\n",
    "print(\"\\nVerification successful: Original and loaded arrays are identical.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a08566-1583-45be-b7e1-11b1adceaf26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
